# -*- coding: utf-8 -*-
"""fireproj_getdata_for_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n3WUupfv24G8UReNH9shPEvtBBGmI99o
"""

#THIS CODE IS A MESS AND IS ONLY FOR HISTORICAL TRACKING AT THIS POINT
# THIS FUNCTION IS FOR DAILY WEATHER DATA

import requests
import pandas as pd
from math import radians, cos, sin, asin, sqrt

# Function to calculate the great circle distance between two points
# on the earth (specified in decimal degrees)
def haversine(lon1, lat1, lon2, lat2):
    # convert decimal degrees to radians
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])

    # haversine formula
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    r = 6371 # Radius of earth in kilometers. Use 3956 for miles
    return c * r

# Function to get the nearest weather station
def get_nearest_station(latitude, longitude):
    stations = {
        '91190022516': (20.88871, -156.43453)#,  # Station 1 coordinates
        #'99738499999': (20.9, -156.47)   # Station 2 coordinates
    }
    distances = {station: haversine(longitude, latitude, *coords) for station, coords in stations.items()}
    return min(distances, key=distances.get)

# Function to retrieve GSOD data for the nearest station
def get_gsod_data(date, latitude, longitude):
    nearest_station = get_nearest_station(latitude, longitude)
    api_token = 'zgMwINDQNvylBcCiyyrymXezccJNOZyp'
    url = f'https://www.ncei.noaa.gov/access/services/data/v1?dataset=global-summary-of-the-day&stations={nearest_station}&startDate={date}&endDate={date}&format=json'
    headers = {'token': api_token}

    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        # Load data into a DataFrame
        df = pd.DataFrame(response.json())
        return df
    else:
        print(f'Failed to retrieve data: {response.status_code}')
        return pd.DataFrame()

# Example usage:
# Provide your API token and call the function with the required date and coordinates.
df = get_gsod_data('2023-11-29', 20.88871, -156.43453)

print(df)

# Function to get fire data from FIRMS API for a specific date and parse out the data for just Maui
def get_fire_data(date):
    firm_api = f"https://firms.modaps.eosdis.nasa.gov//api/area/csv/33c0bb32b80831ae1cb4bb94211611c8/MODIS_NRT/world/1/{date}"
    df_firm = pd.read_csv(firm_api)
    df_firm = df_firm[['acq_date', 'latitude', 'longitude', 'frp']]
    df_firm = df_firm[(df_firm['latitude'] >= 20.500) & (df_firm['latitude'] <= 21.0000)]
    df_firm = df_firm[(df_firm['longitude'] >= -156.5000) & (df_firm['longitude'] <= -156.0000)]
    df_firm = df_firm[df_firm['frp'] > 0]
    df_firm = df_firm.sort_values(by='latitude')
    return df_firm

df2 = get_fire_data('2023-11-26')
print(df2.head())

def combine_fire_and_weather_data(date):
    # Get fire data for the specified date
    fire_data = get_fire_data(date)

    # List to store individual DataFrames for each fire location
    combined_data_list = []

    for index, row in fire_data.iterrows():
        # Get weather data for the nearest station to each fire location
        weather_data = get_gsod_data(date, row['latitude'], row['longitude'])

        # If weather data is found, merge it with the fire data
        if not weather_data.empty:
            # Add fire data columns to the weather data
            for col in ['frp', 'latitude', 'longitude']:
                weather_data[col] = row[col]

            # Append the weather_data DataFrame to the list
            combined_data_list.append(weather_data)

    # Concatenate all DataFrames in the list
    combined_data = pd.concat(combined_data_list, ignore_index=True)

    return combined_data

# Example usage
combined_df = combine_fire_and_weather_data('2023-08-09')
print(combined_df)

#This is to get the past years worth of combined fire and weather data to save as a df
import requests
import pandas as pd
from io import StringIO
from datetime import datetime, timedelta

import time

# Function to get the nearest weather station
def get_nearest_station(latitude, longitude):
    stations = {
        '91190022516': (20.88871, -156.43453),  # Station 1 coordinates
          # Note that if there is no fires on a given day, to use station 1's data because
          # it is closer to the center of Maui and contains more data we could use down
          # the road
        '99738499999': (20.9, -156.47)   # Station 2 coordinates
    }
    distances = {station: haversine(longitude, latitude, *coords) for station, coords in stations.items()}
    return min(distances, key=distances.get)

# Function to generate a list of dates for the past 365 days
def generate_past_year_dates():
    end_date = datetime.today()
    start_date = end_date - timedelta(days=365)
    return [start_date + timedelta(days=i) for i in range((end_date - start_date).days)]


# Function to retrieve GSOD data for the nearest station
def get_gsod_data(date, latitude, longitude):
    nearest_station = get_nearest_station(latitude, longitude)
    api_token = 'zgMwINDQNvylBcCiyyrymXezccJNOZyp'
    url = f'https://www.ncei.noaa.gov/access/services/data/v1?dataset=global-summary-of-the-day&stations={nearest_station}&startDate={date}&endDate={date}&format=json'
    headers = {'token': api_token}
    response = requests.get(url, headers=headers)
    if response.status_code == 403 or response.status_code == 503:
        response.raise_for_status()
    if response.status_code != 200:
        print(f'Failed to retrieve data: HTTP {response.status_code}')
        return pd.DataFrame()
    return pd.DataFrame(response.json())

# Function to get fire data from FIRMS API for a specific date and parse out the data for just Maui
def get_fire_data(date):
    firm_api = f"https://firms.modaps.eosdis.nasa.gov//api/area/csv/33c0bb32b80831ae1cb4bb94211611c8/MODIS_NRT/world/1/{date}"
    response = requests.get(firm_api)
    if response.status_code == 403 or response.status_code == 503:
        response.raise_for_status()
    if response.status_code != 200:
        print(f"Error fetching fire data: HTTP {response.status_code}")
        return pd.DataFrame()

    # Use StringIO to read the CSV data
    df_firm = pd.read_csv(StringIO(response.text))
    df_firm = df_firm[['acq_date', 'latitude', 'longitude', 'frp']]
    df_firm = df_firm[(df_firm['latitude'] >= 20.500) & (df_firm['latitude'] <= 21.0000)]
    df_firm = df_firm[(df_firm['longitude'] >= -156.5000) & (df_firm['longitude'] <= -156.0000)]
    df_firm = df_firm[df_firm['frp'] > 0]
    df_firm = df_firm.sort_values(by='latitude')
    return df_firm


#This is a modified version of the previous function so that we can pull a years worth
# of data but stop if things get overloaded
def combine_fire_and_weather_data(date):
    combined_data_list = []

    # Define the columns for your DataFrame
    columns = ['date', 'SNDP', 'WDSP', 'STATION', 'DEWP', 'MAX', 'VISIB', 'PRCP', 'GUST', 'STP', 'MIN', 'TEMP', 'SLP', 'MXSPD', 'FRSHTT', 'frp', 'latitude', 'longitude']

    # Get fire data for the specified date
    fire_data = get_fire_data(date)

    if fire_data.empty:
        # Coordinates of Station 1
        station_1_lat, station_1_lon = 20.88871, -156.43453

        # Fetch weather data for Station 1
        weather_data = get_gsod_data(date, station_1_lat, station_1_lon)

        if not weather_data.empty:
            weather_data['date'] = date
            weather_data = weather_data.assign(frp=pd.NA, latitude=station_1_lat, longitude=station_1_lon)
            combined_data_list.append(weather_data[columns])
        else:
            # Create a blank row with the date and NaN for other columns if no weather data is available
            blank_row = {col: pd.NA for col in columns}
            blank_row['date'] = date
            combined_data_list.append(pd.DataFrame([blank_row], columns=columns))
    else:
        for index, row in fire_data.iterrows():
            # Get weather data for the nearest station to each fire location
            weather_data = get_gsod_data(date, row['latitude'], row['longitude'])

            if not weather_data.empty:
                # Combine fire data with weather data
                weather_data['date'] = date
                for col in ['frp', 'latitude', 'longitude']:
                    weather_data[col] = row[col]
                combined_data_list.append(weather_data[columns])
            else:
                # Create a blank row with fire data and NaN for weather data columns
                fire_data_row = {**row.to_dict(), 'date': date}
                blank_row = {col: fire_data_row.get(col, pd.NA) for col in columns}
                combined_data_list.append(pd.DataFrame([blank_row], columns=columns))

    combined_data = pd.concat(combined_data_list, ignore_index=True) if combined_data_list else pd.DataFrame(columns=columns)
    return combined_data


def save_data(df, filepath):
    df.to_csv(filepath, index=False)

# Generate dates
# This is the first start date
#restart_date = datetime(2022, 11, 29)
restart_date = datetime(2023, 9, 20)

dates = generate_past_year_dates()

# DataFrame to store all combined data
all_combined_data = pd.DataFrame()

# Path to save the data
file_path = '/content/drive/My Drive/combined_fire_weather_data.csv'

for date in dates:
    if date <= restart_date:
        continue

    try:
        date_str = date.strftime('%Y-%m-%d')
        daily_combined_data = combine_fire_and_weather_data(date_str)
        all_combined_data = pd.concat([all_combined_data, daily_combined_data], ignore_index=True)
        save_data(all_combined_data, file_path)

    except requests.exceptions.HTTPError as e:
        if e.response.status_code in [503, 403]:
            print(f"API error {e.response.status_code} on {date_str}: {e}")
            break
    except Exception as e:
        print(f"General error on {date_str}: {e}")

    time.sleep(5)

# Final save in case the loop completes without errors
save_data(all_combined_data, file_path)

# Save to Google Drive - only need to run this part once
#from google.colab import drive
#drive.mount('/content/drive')

# Adjust the path if your file is in a specific folder in Google Drive
df = pd.read_csv('/content/drive/My Drive/combined_fire_weather_data.csv')
print(df.head())

from google.colab import drive
import pandas as pd
import numpy as np

# Mount your Google Drive - only have to run once
#drive.mount('/content/drive')

# Define the file path
file_path = '/content/drive/My Drive/combined_fire_weather_data_pastyr.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)


#How can I add a column to the combined_fire_weather_data_pastyr.csv file
#that is 1 if there was an FRP >0 the day before and 0 if there was not
# and call it past_day_fire_bin
df['date'] = pd.to_datetime(df['date'])

# Sort the DataFrame by date
df.sort_values('date', inplace=True)

# Create a binary column for FRP > 0, renamed to 'past_day_fire_bin'
df['frp_gt_0'] = (df['frp'] > 0).astype(int)

# Group by date and check if there was any FRP > 0 for that date
daily_frp = df.groupby('date')['frp_gt_0'].max()

# Shift the series by one day to represent the previous day's status
previous_day_frp = daily_frp.shift(1).fillna(0).astype(int)  # Convert to int to remove decimal places

# Merge this information back into the original DataFrame
df = df.merge(previous_day_frp.rename('past_day_fire_bin'), on='date')

# Ensure 'past_day_fire_bin' is an integer
df['past_day_fire_bin'] = df['past_day_fire_bin'].astype(int)

# Drop the temporary 'frp_gt_0' column
df.drop('frp_gt_0', axis=1, inplace=True)


#I then need to make any blanks NaN
# Replace blanks with NaN
df.replace('', np.nan, inplace=True)


#check that it all is working properly
pd.set_option('display.max_rows', None)

print(df[['date', 'past_day_fire_bin']])

pd.reset_option('display.max_rows')


filtered_df = df[df['date'] == '2022-12-31']

# Print the filtered DataFrame
print(filtered_df)



#I then need to overwrite combined_fire_weather_data_pastyr.csv in google drive
# to the version with this extra flag
# Path to your file
file_path = '/content/drive/My Drive/combined_fire_weather_data_pastyr.csv'

# Save the DataFrame as a CSV, replacing the old file
df.to_csv(file_path, index=False)

#double checking that everything worked
file_path = '/content/drive/My Drive/combined_fire_weather_data_pastyr.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

filtered_df = df[df['date'] == '2022-12-31']

# Print the filtered DataFrame
print(filtered_df)

# Group by date and check if any non-NaN FRP values exist for each date
fire_exist_series = df.groupby('date')['frp'].apply(lambda x: 1 if x.notna().any() else 0)

# Convert the Series to a DataFrame
fire_exist_df = fire_exist_series.reset_index(name='Fire_exist')

# Convert the 'date' column to datetime if it's not already
fire_exist_df['date'] = pd.to_datetime(fire_exist_df['date'])

# Sort the DataFrame by the 'date' column
fire_exist_df = fire_exist_df.sort_values(by='date')

# Display the first few rows of the new DataFrame
print(fire_exist_df.head())

#check that it all is working properly
pd.set_option('display.max_rows', None)

# Print the entire DataFrame
print(fire_exist_df)

# Reset the option back to default if needed
pd.reset_option('display.max_rows')

#How can I export the fire_dates_df dataset to a csv?

# only needs ran once to connect drive
from google.colab import drive
drive.mount('/content/drive')


# Define the file path
file_path = '/content/drive/My Drive/fire_weather_dates_ML_binary.csv'

# Export the DataFrame to a CSV file
fire_exist_df.to_csv(file_path, index=False)

print(f"File saved to {file_path}")

#how many dates have Fire_exist = 1?


# Filter the DataFrame to include only rows where 'Fire_exist' equals 1
fire_dates_df = fire_exist_df[fire_exist_df['Fire_exist'] == 1]

# Display the filtered DataFrame
print(fire_dates_df)

#Why is PRCP = 0 for all of my data and does DEWP ever change either?
# It's because it's blank for all of that weather station 99738499999
# Here is what I'm going to do
# It looks like the station data at 91190022516 is always better since
# station 99738499999 is missing a lot of the variables we want
#lets replace all the variables in our past year data
# we'll need to revise our get closest weather data station to only look
# at collecting the data from 91190022516 and we can write in the paper
# that we want futher work to look at pulling from multiple weather
# stations when data is missing


# only needs ran once to connect drive
#from google.colab import drive
#drive.mount('/content/drive')

# Adjust the file path according to your Google Drive folder structure
file_path = '/content/drive/My Drive/weather_91190022516_pastyr.json'

# Read the JSON file into a DataFrame
df = pd.read_json(file_path)

# Define the columns for your DataFrame
columns_to_keep = ['DATE', 'WDSP', 'STATION', 'DEWP', 'PRCP',  'TEMP'] #,  'frp', 'latitude', 'longitude']

# Select only the specified columns
df = df[columns_to_keep]

# Display the DataFrame to verify the changes
print(df.head())


#read in combined_fire_weather_data_pastyr.csv and narrow down to just date, frp, latitude and longitude

file_path = '/content/drive/My Drive/combined_fire_weather_data_pastyr.csv'

# Read the CSV file into a DataFrame
df_fire = pd.read_csv(file_path)

columns_to_keep = [ 'date', 'frp', 'latitude', 'longitude','past_day_fire_bin']

# Select only the specified columns
df_fire = df_fire[columns_to_keep]

print(df_fire.head())

# Convert the 'DATE' column in df to datetime if it's not already
df['DATE'] = pd.to_datetime(df['DATE'])

# Convert the 'date' column in df_fire to datetime if it's not already
df_fire['date'] = pd.to_datetime(df_fire['date'])

# Rename the 'date' column in df_fire to 'DATE'
df_fire.rename(columns={'date': 'DATE'}, inplace=True)


# Merge the two dataframes on 'DATE'
combined_df = pd.merge(df, df_fire, on='DATE', how='outer')

# Display the combined DataFrame
print(combined_df.head())

# Define the file path
file_path = '/content/drive/My Drive/combined_fire_weather_pastyr_update.csv'

# Export the DataFrame to a CSV file
combined_df.to_csv(file_path, index=False)

print(f"File saved to {file_path}")

import folium
import pandas as pd
from geopy.distance import geodesic
import branca.colormap as cm


# Function to calculate square corners around a point
def get_square_corners(lat, lon, distance_km=0.5):
    # Calculate the four corners of a square centered at (lat, lon)
    # distance_km is the half-side of the square, defaulting to 0.5 km for a 1km x 1km square
    center = (lat, lon)
    north = geodesic(kilometers=distance_km).destination(center, bearing=0)
    east = geodesic(kilometers=distance_km).destination(center, bearing=90)
    south = geodesic(kilometers=distance_km).destination(center, bearing=180)
    west = geodesic(kilometers=distance_km).destination(center, bearing=270)

    ne = (north.latitude, east.longitude)
    se = (south.latitude, east.longitude)
    sw = (south.latitude, west.longitude)
    nw = (north.latitude, west.longitude)

    return [ne, se, sw, nw, ne]  # Return coordinates to form a closed square

# Function to determine circle size based on prediction score
def get_circle_radius(prediction_score):
    # This is a simple linear scaling. You can adjust the scaling factor as needed.
    return prediction_score * 10000  # for example, 10 meters per score unit


# Function to create a map with fire locations marked as squares
def create_fire_map(date, ml_predictions):
    df_fires = get_fire_data(date)
    maui_center = [20.7984, -156.3319]
    maui_map = folium.Map(location=maui_center, tiles='CartoDB dark_matter', zoom_start=10)

    # Check if there are fire spots
    if not df_fires.empty:
        # Create a red color scale for the FRP values
        max_frp = df_fires['frp'].max()
        min_frp = df_fires['frp'].min()

        # Define a color scale from light red to dark red using hexadecimal colors
        colormap = cm.LinearColormap(['#ffcccc', '#ff0000'], vmin=min_frp, vmax=max_frp)

        for _, row in df_fires.iterrows():
            corners = get_square_corners(row['latitude'], row['longitude'])
            color = colormap(row['frp'])
            folium.Polygon(
                locations=corners,
                color=color,
                fill=True,
                fill_color=color
            ).add_to(maui_map)

        # Add the color scale legend to the map
        colormap.caption = 'Fire Radiative Power (FRP)'
        maui_map.add_child(colormap)
    else:
        # Add a message to the map indicating no fire spots
        folium.Marker(
            location=[20.7984, -156.3319],
            popup="No fire locations on this date",
            icon=folium.Icon(color="blue", icon="info-sign")
        ).add_to(maui_map)

    # Extract the prediction score for the given date
    prediction_score = ml_predictions[ml_predictions['date'] == date]['prediction_score'].max()

    # Draw a circle based on the prediction score
    circle_radius = get_circle_radius(prediction_score)
    folium.Circle(
        location=maui_center,
        radius=circle_radius,
        color='orange',
        fill=True,
        fill_color='orange'
    ).add_to(maui_map)


    return maui_map


ml_predictions = pd.DataFrame({'date': ['2023-08-09', '2023-08-10'], 'prediction_score': [0.5, 0.9]})

# Example usage
fire_map = create_fire_map('2023-08-09', ml_predictions)
fire_map.save('maui_fire_map_with_predictions.html')

fire_map

#I need a way to make a "spread" variable which indicates where a FRP location
# has other FRP locations around it the next day


# Initialize an empty DataFrame to store the combined data for all dates
all_dates_combined_df = pd.DataFrame()

# Iterate over each date in the range and accumulate the data
for single_date in pd.date_range(start='2023-08-08', end='2023-08-10'):
    # Format the date as a string
    date_str = single_date.strftime('%Y-%m-%d')

    # Get combined fire and weather data for this date
    daily_combined_df = combine_fire_and_weather_data(date_str)

    # Concatenate the daily data to the main DataFrame
    all_dates_combined_df = pd.concat([all_dates_combined_df, daily_combined_df], ignore_index=True)

# Now all_dates_combined_df contains the combined data for the specified date range

print(all_dates_combined_df)

# Define 'high' FRP threshold
high_frp_threshold = 1  # Example value, adjust based on your data

# Define a function to determine if high FRP points are nearby
def is_fire_spread(lat, lon, date, df, radius=10):
    # Calculate the next day
    next_day = pd.to_datetime(date) + pd.Timedelta(days=1)

    # Filter DataFrame for the next day
    next_day_df = df[df['date'] == next_day.strftime('%Y-%m-%d')]

    # Check for high FRP points within the specified radius
    for index, row in next_day_df.iterrows():
        if row['frp'] > high_frp_threshold and haversine(lon, lat, row['longitude'], row['latitude']) <= radius:
            return 1
    return 0

# Apply the function to create the 'spread' column
combined_df['spread'] = combined_df.apply(lambda row: is_fire_spread(row['latitude'], row['longitude'], row['DATE'], combined_df), axis=1)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd

# Assuming combined_df is your final DataFrame with all necessary features and the target label
# Example: combined_df = pd.concat([get_fire_data(date) for date in pd.date_range(start='2023-08-08', end='2023-08-11')])


# Split the data into features (X) and target (y)
X = combined_df[['frp', 'TEMP', 'WDSP', 'PRCP']]  # Add or remove features based on your dataset
y = combined_df['spread']  # 'spread' is a binary column indicating whether the fire spread

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))